{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to use with image url\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "from tensorflow_text.python.ops.fast_wordpiece_tokenizer import FastWordpieceTokenizer\n",
    "from keras_nlp.layers import StartEndPacker\n",
    "from tensorflow_text import normalize_utf8\n",
    "import keras_nlp\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import register_keras_serializable \n",
    "\n",
    "PROJECT_DIR = \"./\"\n",
    "\n",
    "file_path = \"./datasets/train_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train tokenizer and construct vocab (same as V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer():\n",
    "\n",
    "    def __init__(self, vocab_path, max_length):\n",
    "        self.packer = StartEndPacker(sequence_length=max_length,pad_value=0)\n",
    "        self.unk_token = '[UNK]'\n",
    "        self.vocabulary = self._get_vocab_list(vocab_path)\n",
    "        self.tokenizer = FastWordpieceTokenizer(\n",
    "            vocab=self.vocabulary,\n",
    "            suffix_indicator='##',\n",
    "            unknown_token=self.unk_token,\n",
    "            support_detokenization=True\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess(text):\n",
    "        text_normalized = normalize_utf8(text, \"NFD\")\n",
    "        text_stripped_accents = tf.strings.regex_replace(text_normalized, r\"\\p{Mn}\", \"\")\n",
    "        lowercase = tf.strings.lower(text_stripped_accents)\n",
    "        return lowercase\n",
    "\n",
    "    def _get_vocab_list(self, vocab_path):\n",
    "        vclist = []\n",
    "\n",
    "        with open(vocab_path, \"r\") as f:\n",
    "            vclist.extend(f.read().splitlines())\n",
    "            seen = set()\n",
    "            vclist = [x for x in vclist if not (x in seen or seen.add(x))]\n",
    "\n",
    "        if self.unk_token not in vclist:\n",
    "            vclist = [vclist[0]] +  [self.unk_token]  + vclist[1:]\n",
    "        return vclist\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self._preprocess(text)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        return self.packer(tokens)\n",
    "\n",
    "    def detokenize(self, tokens):\n",
    "        return self.tokenizer.detokenize(tokens)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(ds, vocab_size, query_or_desc, vocab_file=None):\n",
    "    if query_or_desc == \"title\":\n",
    "        word_piece_ds = ds.map(lambda x: x[\"title\"])\n",
    "    elif query_or_desc == 'desc':\n",
    "        word_piece_ds = ds.map(lambda x: x[\"description\"])\n",
    "\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(10000).prefetch(1000),\n",
    "        vocabulary_output_file=vocab_file,\n",
    "        vocabulary_size=vocab_size,\n",
    "        lowercase=True,\n",
    "        strip_accents=True,\n",
    "        reserved_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    )\n",
    "    return vocab\n",
    "\n",
    "def get_tokenizer(train, vocab_size, csv_size, vocab_type, max_tokens, force_train=False):\n",
    "    vocab_filename = f\"vocab/{csv_size}_{vocab_type}\"\n",
    "    vocab_path = os.path.join(PROJECT_DIR, vocab_filename)\n",
    "    print(vocab_path)\n",
    "    try:\n",
    "        if force_train:\n",
    "            raise FileNotFoundError()\n",
    "\n",
    "        tokenizer = CustomTokenizer(vocab_path, max_tokens)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Training tokenizer...\")\n",
    "        train_word_piece(train, vocab_size, vocab_type, vocab_path)\n",
    "\n",
    "        tokenizer = CustomTokenizer(vocab_path, max_tokens)\n",
    "\n",
    "    final_vocab_size = len(tokenizer.vocabulary)\n",
    "    print(f\"Loaded tokenizer from '{vocab_filename}' with final vocab size: {final_vocab_size:,}\")\n",
    "    print(\"Sample tokens: \", np.random.choice(tokenizer.vocabulary, 20))\n",
    "    return tokenizer, final_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre process image and other input cols (same as V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_image(url, target_size=(320, 320)):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    img.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    delta_w = target_size[0] - img.width\n",
    "    delta_h = target_size[1] - img.height\n",
    "    padding = (\n",
    "        delta_w // 2, \n",
    "        delta_h // 2, \n",
    "        delta_w - (delta_w // 2), \n",
    "        delta_h - (delta_h // 2)\n",
    "        )\n",
    "    \n",
    "    padded_img = ImageOps.expand(img, padding, fill=(0, 0, 0))\n",
    "    \n",
    "    return padded_img\n",
    "\n",
    "def normalize_image(pil_image):\n",
    "    img_array = np.array(pil_image).astype(np.float32) / 255.0\n",
    "\n",
    "    img_tensor = tf.convert_to_tensor(img_array)\n",
    "    img_tensor = tf.expand_dims(img_tensor, axis=0)  \n",
    "    return img_tensor\n",
    "\n",
    "def process_image(url):\n",
    "    def _load_and_process(url_str):\n",
    "        url_decoded = url_str.numpy().decode()\n",
    "        img = resize_and_pad_image(url_decoded)\n",
    "        img_tensor = normalize_image(img)\n",
    "        return img_tensor[0]  # remove batch dim\n",
    "\n",
    "    img = tf.py_function(func=_load_and_process, inp=[url], Tout=tf.float32)\n",
    "    img.set_shape([320, 320, 3])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_cols = [col for col in df.columns if col.startswith(\"condition_\")]\n",
    "df[condition_cols] = df[condition_cols].astype(np.int32)\n",
    "\n",
    "df[\"year\"] = df[\"year\"].astype(np.int32)\n",
    "\n",
    "sin_cos_cols = [\"month_sin\", \"month_cos\", \"day_of_week_sin\", \"day_of_week_cos\"]\n",
    "df[sin_cos_cols] = df[sin_cos_cols].astype(np.float32)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"description\"] = df[\"description\"].astype(str)\n",
    "df[\"image_url\"] = df[\"image_url\"].astype(str)\n",
    "\n",
    "df.fillna(\"\", inplace=True)  # for strings\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices({\n",
    "    \"title\": df[\"title\"].values,\n",
    "    \"description\": df[\"description\"].values,\n",
    "    \"image_url\": df[\"image_url\"].values,\n",
    "    \"condition_BRAND NEW\": df[\"condition_BRAND NEW\"].values,\n",
    "    \"condition_HEAVILY USED\": df[\"condition_HEAVILY USED\"].values,\n",
    "    \"condition_LIGHTLY USED\": df[\"condition_LIGHTLY USED\"].values,\n",
    "    \"condition_LIKE NEW\": df[\"condition_LIKE NEW\"].values,\n",
    "    \"condition_WELL USED\": df[\"condition_WELL USED\"].values,\n",
    "    \"year\": df[\"year\"].values,\n",
    "    \"month_sin\": df[\"month_sin\"].values,\n",
    "    \"month_cos\": df[\"month_cos\"].values,\n",
    "    \"day_of_week_sin\": df[\"day_of_week_sin\"].values,\n",
    "    \"day_of_week_cos\": df[\"day_of_week_cos\"].values,\n",
    "    \"price_log\": df[\"price_log\"].values, # target\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = ds.shuffle(len(ds), seed = 42, reshuffle_each_iteration=False)\n",
    "train_ds_raw = shuffled.take(int(len(ds) * 0.9))\n",
    "test_ds_raw = shuffled.skip(int(len(ds) * 0.9))\n",
    "\n",
    "def cyclical_encode(value, max_value):\n",
    "    sin_val = round(np.sin(2 * np.pi * value / max_value), 2)\n",
    "    cos_val = round(np.cos(2 * np.pi * value / max_value), 2)\n",
    "    return sin_val, cos_val\n",
    "\n",
    "def tokenize(title, description, title_tokenizer, desc_tokenizer):\n",
    "    x = title_tokenizer(title)\n",
    "    y = desc_tokenizer(description)\n",
    "    return {\n",
    "        \"title_tokens\": x,\n",
    "        \"desc_tokens\": y,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputs, title_tokenizer, desc_tokenizer):\n",
    "    title = inputs[\"title\"]\n",
    "    desc = inputs[\"description\"]\n",
    "    img_url = inputs[\"image_url\"]\n",
    "    tokenized = tokenize(title, desc, title_tokenizer, desc_tokenizer)\n",
    "    img_tensor = process_image(img_url)\n",
    "\n",
    "    price = inputs[\"price_log\"]\n",
    "\n",
    "    structured = {\n",
    "        key: tf.cast(inputs[key], tf.float32)\n",
    "        for key in [\n",
    "            \"condition_BRAND NEW\", \"condition_HEAVILY USED\", \"condition_LIGHTLY USED\",\n",
    "            \"condition_LIKE NEW\", \"condition_WELL USED\",\n",
    "            \"year\", \"month_sin\", \"month_cos\", \"day_of_week_sin\", \"day_of_week_cos\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    model_inputs = {\n",
    "        \"image_url\": img_tensor,\n",
    "        \"title\": tokenized[\"title_tokens\"],\n",
    "        \"description\": tokenized[\"desc_tokens\"],\n",
    "        **structured\n",
    "    }\n",
    "\n",
    "    return model_inputs, tf.cast(price, tf.float32)  # model input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller tokens (factor of 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./vocab/train_cleaned.csv_title\n",
      "Loaded tokenizer from 'vocab/train_cleaned.csv_title' with final vocab size: 434\n",
      "Sample tokens:  ['\"' '解' '##!' 'consoles' '种' 'n' '##ore' '6' 'dual' '##版' '##x' 'brand'\n",
      " '##m' '❄' 'adapter' '##👾' 'selling' 'nintendo' 'all' 'selling']\n",
      "./vocab/train_cleaned.csv_desc\n",
      "Loaded tokenizer from 'vocab/train_cleaned.csv_desc' with final vocab size: 2,321\n",
      "Sample tokens:  ['##\\U0001fa77' '##us' 'call' '##💽' '\\U0001fa77' 'together' 'warfare'\n",
      " '##ス' '##😫' 'fix' 'thanks' '##❗' '##🏦' '##!' 'xbox' 'block' '##view'\n",
      " 'was' 'dead' '{']\n"
     ]
    }
   ],
   "source": [
    "csv_size = \"train_cleaned.csv\"\n",
    "MAX_TITLE_VOCAB_SIZE = 500\n",
    "MAX_DESC_VOCAB_SIZE = 3000\n",
    "\n",
    "vocab_type = \"title\"\n",
    "title_max_tokens = 8\n",
    "title_tokenizer, TITLE_VOCAB_SIZE = get_tokenizer(\n",
    "    train_ds_raw, MAX_TITLE_VOCAB_SIZE, csv_size, vocab_type, title_max_tokens, force_train=False)\n",
    "\n",
    "vocab_type = \"desc\"\n",
    "desc_max_tokens = 16\n",
    "desc_tokenizer, DESC_VOCAB_SIZE = get_tokenizer(\n",
    "    train_ds_raw, MAX_DESC_VOCAB_SIZE, csv_size, vocab_type, desc_max_tokens, force_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[105 143 162   0   0   0   0   0]], shape=(1, 8), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 413  366   40   58  811 1591 1248  360  358   43 1575  892  464   15\n",
      "     0    0]], shape=(1, 16), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(title_tokenizer([\"xbox for sale\"]))\n",
    "print(desc_tokenizer([\"This is a sample description for the dataset.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Title\n",
      "Text:\t\t PlayStation 3 Console\n",
      "Tokens:\t\t tf.Tensor([111  20 115   0   0   0   0   0], shape=(8,), dtype=int64)\n",
      "Recovered:\t playstation 3 console [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      ">> Description\n",
      "Text:\t\t nan\n",
      "Tokens:\t\t tf.Tensor([453   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(16,), dtype=int64)\n",
      "Recovered:\t nan [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_sample(text, tokenizer, type_):\n",
    "    print(f'>> {type_}')\n",
    "    tokens = tokenizer(text)\n",
    "    print(\"Text:\\t\\t\", text)\n",
    "    print(\"Tokens:\\t\\t\", tokens)\n",
    "    print(\"Recovered:\\t\", tokenizer.detokenize(tokens).numpy().decode())\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "sample = df.sample(1)\n",
    "print_sample(sample['title'].values[0], title_tokenizer, 'Title')\n",
    "print_sample(sample['description'].values[0], desc_tokenizer, 'Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[262 310  33 137  34 243 137 317]], shape=(1, 8), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 537  413  746  486 1924  599  607  514  672    0    0    0    0    0\n",
      "     0    0]], shape=(1, 16), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# check tokenizer outputs, sanity check\n",
    "\n",
    "print(title_tokenizer(['gaming can be cheap']))\n",
    "print(desc_tokenizer(['buy this amazing tool super cheap']))\n",
    "\n",
    "# tf.Tensor([[262 310  33 137  34 243 137 317]], shape=(1, 8), dtype=int64)\n",
    "# tf.Tensor(\n",
    "# [[ 537  413  746  486 1924  599  607  514  672    0    0    0    0    0\n",
    "#      0    0]], shape=(1, 16), dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds_raw.map(\n",
    "    lambda x: preprocess(x, title_tokenizer, desc_tokenizer), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds_raw.map(\n",
    "    lambda x: preprocess(x, title_tokenizer, desc_tokenizer), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of price_log: 4.96927619525621\n"
     ]
    }
   ],
   "source": [
    "total = 0.0\n",
    "count = 0\n",
    "\n",
    "for _, price_log_batch in train_ds:\n",
    "    total += tf.reduce_sum(price_log_batch).numpy()\n",
    "    count += price_log_batch.shape[0]\n",
    "\n",
    "mean_price_log = total / count\n",
    "print(\"Mean of price_log:\", mean_price_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 0.7929205028543173\n",
      "Baseline MSE: 1.2709249491841683\n",
      "Baseline RMSE: 1.1273530721048168\n"
     ]
    }
   ],
   "source": [
    "mae_total = 0.0\n",
    "mse_total = 0.0\n",
    "n = 0\n",
    "\n",
    "for _, price_log_batch in test_ds:\n",
    "    batch_size = price_log_batch.shape[0]\n",
    "    baseline_preds = tf.ones_like(price_log_batch) * mean_price_log\n",
    "    \n",
    "    mae_total += tf.reduce_sum(tf.abs(price_log_batch - baseline_preds)).numpy()\n",
    "    mse_total += tf.reduce_sum(tf.square(price_log_batch - baseline_preds)).numpy()\n",
    "    n += batch_size\n",
    "\n",
    "mae = mae_total / n\n",
    "mse = mse_total / n\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Baseline MAE: {mae}\")\n",
    "print(f\"Baseline MSE: {mse}\")\n",
    "print(f\"Baseline RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nV3 tries to use far less params by the following ways\\n\\n1. Under image encoder\\n1.1 use Gloval Avg Pool instead of Flatten\\n1.2 L2 regularization \\n\\n2. Under Price Predictor class\\n2.1 higher dropout rate\\n2.2 embedding dims for title and desc reduced to 64 (from 128 earlier)\\n2.3 L2 regularization\\n2.4 other features passed through a dense layer, before concat \\n\\nSimplified final layer : 32 > 1\\nFor V1 it was : 256 > 128 > 1\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "V3 tries to use far less params by the following ways\n",
    "\n",
    "1. Under image encoder\n",
    "1.1 use Gloval Avg Pool instead of Flatten\n",
    "1.2 L2 regularization \n",
    "\n",
    "2. Under Price Predictor class\n",
    "2.1 higher dropout rate\n",
    "2.2 embedding dims for title and desc reduced to 64 (from 128 earlier)\n",
    "2.3 L2 regularization\n",
    "2.4 other features passed through a dense layer, before concat \n",
    "\n",
    "Simplified final layer : 32 > 1\n",
    "For V1 it was : 256 > 128 > 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class CNNImageEncoder(tf.keras.Model):\n",
    "    def __init__(self, activation='relu', kernel_size=(3, 3), pool_size=(2, 2), **kwargs):\n",
    "        super(CNNImageEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.cnn_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, kernel_size, activation=activation,\n",
    "                                   input_shape=(320, 320, 3),\n",
    "                                   kernel_regularizer=regularizers.l2(1e-4)),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size, strides=(2, 2)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(64, kernel_size, activation=activation,\n",
    "                                   kernel_regularizer=regularizers.l2(1e-4)),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size, strides=(2, 2)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(128, kernel_size, activation=activation,\n",
    "                                   kernel_regularizer=regularizers.l2(1e-4)),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size, strides=(2, 2)),\n",
    "\n",
    "            # Replace flatten with global pooling to reduce params, compared to V1\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        ])\n",
    "\n",
    "    def call(self, image_inputs):\n",
    "        return self.cnn_layers(image_inputs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.cnn_layers.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"activation\": self.activation,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"pool_size\": self.pool_size,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "DROPOUT = 0.3 # higher dropout as compared with V1\n",
    "\n",
    "@register_keras_serializable()\n",
    "class PricePredictor(tf.keras.Model):\n",
    "    def __init__(self, title_vocab_size=434, desc_vocab_size=2321,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.title_vocab_size = title_vocab_size or 434\n",
    "        self.desc_vocab_size = desc_vocab_size or 2321\n",
    "        \n",
    "        self.image_encoder = CNNImageEncoder()\n",
    "\n",
    "        self.title_embedding_layer = tf.keras.layers.Embedding(title_vocab_size, 64, mask_zero=True)\n",
    "        self.desc_embedding_layer = tf.keras.layers.Embedding(desc_vocab_size, 64, mask_zero=True)\n",
    "\n",
    "        self.title_dense_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "            tf.keras.layers.Dropout(DROPOUT)\n",
    "        ])\n",
    "\n",
    "        self.desc_dense_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "            tf.keras.layers.Dropout(DROPOUT)\n",
    "        ])\n",
    "\n",
    "        self.other_features_processing = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "            tf.keras.layers.Dropout(DROPOUT)\n",
    "        ])\n",
    "\n",
    "        # Final head with reduced complexity\n",
    "        self.final_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "            tf.keras.layers.Dropout(DROPOUT),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        image_input = inputs[\"image_url\"]\n",
    "        title_tokens = inputs[\"title\"]\n",
    "        desc_tokens = inputs[\"description\"]\n",
    "\n",
    "        image_features = self.image_encoder(image_input)\n",
    "\n",
    "        title_embeddings = self.title_embedding_layer(title_tokens)\n",
    "        title_features = self.title_dense_layers(title_embeddings)\n",
    "\n",
    "        desc_embeddings = self.desc_embedding_layer(desc_tokens)\n",
    "        desc_features = self.desc_dense_layers(desc_embeddings)\n",
    "\n",
    "        other_raw_features = tf.keras.layers.concatenate([\n",
    "            tf.expand_dims(inputs[\"condition_BRAND NEW\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"condition_HEAVILY USED\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"condition_LIGHTLY USED\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"condition_LIKE NEW\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"condition_WELL USED\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"year\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"month_sin\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"month_cos\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"day_of_week_sin\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"day_of_week_cos\"], axis=1),\n",
    "        ])\n",
    "\n",
    "        other_features = self.other_features_processing(other_raw_features)\n",
    "\n",
    "        concatenated_features = tf.keras.layers.concatenate([\n",
    "            image_features,\n",
    "            title_features,\n",
    "            desc_features,\n",
    "            other_features\n",
    "        ])\n",
    "\n",
    "        return self.final_layers(concatenated_features)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"title_vocab_size\": self.title_vocab_size,\n",
    "            \"desc_vocab_size\": self.desc_vocab_size,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduce the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(\"./datasets/validation.csv\")\n",
    "\n",
    "valid_df = pd.get_dummies(valid_df, columns=['condition'], dtype=int)\n",
    "valid_df['date_sold'] = pd.to_datetime(valid_df['date_sold'])\n",
    "valid_df['year'] = valid_df['date_sold'].dt.year - 2024 # as we normalize with train set\n",
    "valid_df['month_sin'], valid_df['month_cos'] = zip(\n",
    "    *valid_df['date_sold'].dt.month.apply(lambda x: cyclical_encode(x, 12))\n",
    "    )\n",
    "valid_df['day_of_week_sin'], valid_df['day_of_week_cos'] = zip(\n",
    "    *valid_df['date_sold'].dt.dayofweek.apply(lambda x: cyclical_encode(x, 7))\n",
    "    )\n",
    "valid_df.drop('date_sold', axis=1, inplace=True)\n",
    "\n",
    "# convert target to log price\n",
    "valid_df['price_log'] = np.log1p(valid_df['price'])\n",
    "valid_df.drop('price', axis=1, inplace=True)\n",
    "\n",
    "valid_df[\"title\"] = valid_df[\"title\"].astype(str)\n",
    "valid_df[\"description\"] = valid_df[\"description\"].astype(str)\n",
    "valid_df[\"image_url\"] = valid_df[\"image_url\"].astype(str)\n",
    "\n",
    "valid_df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds_raw = tf.data.Dataset.from_tensor_slices({\n",
    "    \"title\": valid_df[\"title\"].values,\n",
    "    \"description\": valid_df[\"description\"].values,\n",
    "    \"image_url\": valid_df[\"image_url\"].values,\n",
    "    \"condition_BRAND NEW\": valid_df[\"condition_BRAND NEW\"].values,\n",
    "    \"condition_HEAVILY USED\": valid_df[\"condition_HEAVILY USED\"].values,\n",
    "    \"condition_LIGHTLY USED\": valid_df[\"condition_LIGHTLY USED\"].values,\n",
    "    \"condition_LIKE NEW\": valid_df[\"condition_LIKE NEW\"].values,\n",
    "    \"condition_WELL USED\": valid_df[\"condition_WELL USED\"].values,\n",
    "    \"year\": valid_df[\"year\"].values,\n",
    "    \"month_sin\": valid_df[\"month_sin\"].values,\n",
    "    \"month_cos\": valid_df[\"month_cos\"].values,\n",
    "    \"day_of_week_sin\": valid_df[\"day_of_week_sin\"].values,\n",
    "    \"day_of_week_cos\": valid_df[\"day_of_week_cos\"].values,\n",
    "    \"price_log\": valid_df[\"price_log\"].values, \n",
    "})\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "valid_ds = valid_ds_raw.map(\n",
    "    lambda x: preprocess(x, title_tokenizer, desc_tokenizer), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "TITLE_VOCAB_SIZE = len(title_tokenizer.vocabulary)\n",
    "DESC_VOCAB_SIZE = len(desc_tokenizer.vocabulary)\n",
    "\n",
    "model = PricePredictor(\n",
    "    title_vocab_size=TITLE_VOCAB_SIZE,\n",
    "    desc_vocab_size=DESC_VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n",
      "2321\n"
     ]
    }
   ],
   "source": [
    "print(model.title_vocab_size)\n",
    "print(model.desc_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"price_predictor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"price_predictor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ cnn_image_encoder               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CNNImageEncoder</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">93,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">318</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">318</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ max_pooling2d           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">159</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">159</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ max_pooling2d_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ max_pooling2d_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ global_average_pooling2d        │                        │               │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ global_average_pooling1d   │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ global_average_pooling1d_1 │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ cnn_image_encoder               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mCNNImageEncoder\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ sequential (\u001b[38;5;33mSequential\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m93,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ conv2d (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m318\u001b[0m, \u001b[38;5;34m318\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ max_pooling2d           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m159\u001b[0m, \u001b[38;5;34m159\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ max_pooling2d_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └ max_pooling2d_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│       └                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ global_average_pooling2d        │                        │               │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ global_average_pooling1d   │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (\u001b[38;5;33mDense\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (\u001b[38;5;33mDropout\u001b[0m)          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_2 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ global_average_pooling1d_1 │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_3 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_4 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,248</span> (364.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,248\u001b[0m (364.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,248</span> (364.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,248\u001b[0m (364.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to see what we feed into the CNN part of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Show 10 sample images\n",
    "# count = 0\n",
    "# for batch in train_ds.take(5):  # Adjust if batch_size is large/small\n",
    "#     images = batch[0][\"image_url\"]\n",
    "#     for i in range(images.shape[0]):\n",
    "#         if count >= 10:\n",
    "#             break\n",
    "#         img = images[i].numpy()\n",
    "\n",
    "#         # Fix: Rescale if float\n",
    "#         if img.dtype == np.float32 and np.max(img) <= 1.0:\n",
    "#             img_vis = (img * 255).astype(\"uint8\")\n",
    "#         elif img.dtype == np.float32:\n",
    "#             img_vis = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "#         else:\n",
    "#             img_vis = img.astype(\"uint8\")\n",
    "\n",
    "#         plt.figure()\n",
    "#         plt.imshow(img_vis)  # or just img if dtype is float32\n",
    "#         plt.title(f\"Sample {count}\")\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "\n",
    "#         # Also print basic stats\n",
    "#         print(f\"Sample {count}: shape={img.shape}, min={np.min(img)}, max={np.max(img)}, mean={np.mean(img)}\")\n",
    "\n",
    "#         count += 1\n",
    "#     if count >= 10:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V3 ./training_log_V3.csv\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "# change model name\n",
    "model_name = \"V3\"\n",
    "\n",
    "log_file_path = os.path.join(PROJECT_DIR, f\"training_log_{model_name}.csv\")\n",
    "csv_logger = CSVLogger(log_file_path)\n",
    "\n",
    "print(model_name, log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 2s/step - loss: 0.3631 - mean_absolute_error: 0.3859 - val_loss: 0.3852 - val_mean_absolute_error: 0.4746\n",
      "Epoch 2/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 2s/step - loss: 0.3556 - mean_absolute_error: 0.3880 - val_loss: 0.4171 - val_mean_absolute_error: 0.4974\n",
      "Epoch 3/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 2s/step - loss: 0.3384 - mean_absolute_error: 0.3858 - val_loss: 0.4387 - val_mean_absolute_error: 0.5099\n",
      "Epoch 4/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 2s/step - loss: 0.3163 - mean_absolute_error: 0.3751 - val_loss: 0.4532 - val_mean_absolute_error: 0.5130\n",
      "Epoch 5/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 2s/step - loss: 0.2905 - mean_absolute_error: 0.3593 - val_loss: 0.4642 - val_mean_absolute_error: 0.5102\n",
      "Epoch 6/10\n",
      "\u001b[1m  8/117\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:54\u001b[0m 2s/step - loss: 0.2350 - mean_absolute_error: 0.3266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 22:10:57.692550: W tensorflow/core/framework/op_kernel.cc:1828] UNKNOWN: UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x3504ce1b0>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    return func(device, token, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n",
      "    outputs = self._call(device, args)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n",
      "    ret = self._func(*args)\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/var/folders/1v/c5zndyld3tb4l5g9x45prpbr0000gp/T/__autograph_generated_filejkyzt8g5.py\", line 17, in _load_and_process\n",
      "    img = ag__.converted_call(ag__.ld(resize_and_pad_image), (ag__.ld(url_decoded),), None, fscope_1)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n",
      "    return f(*args)\n",
      "           ^^^^^^^^\n",
      "\n",
      "  File \"/var/folders/1v/c5zndyld3tb4l5g9x45prpbr0000gp/T/ipykernel_1636/2429457477.py\", line 3, in resize_and_pad_image\n",
      "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/PIL/Image.py\", line 3580, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x3504ce1b0>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 10/117\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:46\u001b[0m 2s/step - loss: 0.2345 - mean_absolute_error: 0.3268Training failed with error: Graph execution error:\n",
      "\n",
      "Detected at node EagerPyFunc defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x3504ce1b0>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    return func(device, token, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n",
      "    outputs = self._call(device, args)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n",
      "    ret = self._func(*args)\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/var/folders/1v/c5zndyld3tb4l5g9x45prpbr0000gp/T/__autograph_generated_filejkyzt8g5.py\", line 17, in _load_and_process\n",
      "    img = ag__.converted_call(ag__.ld(resize_and_pad_image), (ag__.ld(url_decoded),), None, fscope_1)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n",
      "    return f(*args)\n",
      "           ^^^^^^^^\n",
      "\n",
      "  File \"/var/folders/1v/c5zndyld3tb4l5g9x45prpbr0000gp/T/ipykernel_1636/2429457477.py\", line 3, in resize_and_pad_image\n",
      "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/PIL/Image.py\", line 3580, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x3504ce1b0>\n",
      "\n",
      "\n",
      "\t [[{{node EagerPyFunc}}]]\n",
      "\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_5005]\n",
      "Retrying in 10 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 22:11:01.610358: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x3504ce1b0>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    return func(device, token, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n",
      "    outputs = self._call(device, args)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n",
      "    ret = self._func(*args)\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/var/folders/1v/c5zndyld3tb4l5g9x45prpbr0000gp/T/__autograph_generated_filejkyzt8g5.py\", line 17, in _load_and_process\n",
      "    img = ag__.converted_call(ag__.ld(resize_and_pad_image), (ag__.ld(url_decoded),), None, fscope_1)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n",
      "    return f(*args)\n",
      "           ^^^^^^^^\n",
      "\n",
      "  File \"/var/folders/1v/c5zndyld3tb4l5g9x45prpbr0000gp/T/ipykernel_1636/2429457477.py\", line 3, in resize_and_pad_image\n",
      "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/PIL/Image.py\", line 3580, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x3504ce1b0>\n",
      "\n",
      "\n",
      "\t [[{{node EagerPyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 2s/step - loss: 0.2523 - mean_absolute_error: 0.3277 - val_loss: 0.4826 - val_mean_absolute_error: 0.5063\n",
      "Epoch 2/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 2s/step - loss: 0.2294 - mean_absolute_error: 0.3119 - val_loss: 0.5178 - val_mean_absolute_error: 0.5113\n",
      "Epoch 3/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 2s/step - loss: 0.2073 - mean_absolute_error: 0.2955 - val_loss: 0.5648 - val_mean_absolute_error: 0.5219\n",
      "Epoch 4/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 2s/step - loss: 0.1875 - mean_absolute_error: 0.2809 - val_loss: 0.6057 - val_mean_absolute_error: 0.5327\n",
      "Epoch 5/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 2s/step - loss: 0.1702 - mean_absolute_error: 0.2666 - val_loss: 0.6515 - val_mean_absolute_error: 0.5468\n",
      "Epoch 6/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 4s/step - loss: 0.1604 - mean_absolute_error: 0.2573 - val_loss: 0.6827 - val_mean_absolute_error: 0.5451\n",
      "Epoch 7/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 2s/step - loss: 0.1543 - mean_absolute_error: 0.2513 - val_loss: 0.6729 - val_mean_absolute_error: 0.5508\n",
      "Epoch 8/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 2s/step - loss: 0.1513 - mean_absolute_error: 0.2498 - val_loss: 0.6996 - val_mean_absolute_error: 0.5527\n",
      "Epoch 9/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 2s/step - loss: 0.1479 - mean_absolute_error: 0.2472 - val_loss: 0.6638 - val_mean_absolute_error: 0.5328\n",
      "Epoch 10/10\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 2s/step - loss: 0.1422 - mean_absolute_error: 0.2414 - val_loss: 0.6820 - val_mean_absolute_error: 0.5555\n",
      "Training completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# EPOCHS = 10\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# model.fit(\n",
    "#     train_ds, \n",
    "#     epochs=EPOCHS,\n",
    "#     validation_data=valid_ds,\n",
    "#     callbacks=[csv_logger],\n",
    "# )\n",
    "\n",
    "import time\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_ds,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=valid_ds,\n",
    "            callbacks=[csv_logger],\n",
    "        )\n",
    "        print(\"Training completed successfully.\")\n",
    "        break  # exit loop if training is successful\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {e}\")\n",
    "        print(\"Retrying in 10 seconds...\")\n",
    "        time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aryan/Downloads/ds_assignment_2025\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "model.save(\"price_predictor_V3.keras\")\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_features(condition_str):\n",
    "    all_conditions = [\"BRAND NEW\", \"HEAVILY USED\", \"LIGHTLY USED\", \"LIKE NEW\", \"WELL USED\"]\n",
    "    cond_dict = {f\"condition_{cond}\": 0 for cond in all_conditions}\n",
    "    if condition_str.upper() in [c.upper() for c in all_conditions]:\n",
    "        cond_dict[f\"condition_{condition_str.upper()}\"] = 1\n",
    "    return cond_dict\n",
    "\n",
    "def prepare_inference_inputs(title, description, image_url, date_sold_str, condition_str, title_tokenizer, desc_tokenizer):\n",
    "    dt = pd.to_datetime(date_sold_str)\n",
    "    \n",
    "    # Cyclical features\n",
    "    structured_features = {}\n",
    "    structured_features[\"year\"] = dt.year - 2024\n",
    "    structured_features[\"month_sin\"], structured_features[\"month_cos\"] = cyclical_encode(dt.month, 12)\n",
    "    structured_features[\"day_of_week_sin\"], structured_features[\"day_of_week_cos\"] = cyclical_encode(dt.dayofweek, 7)\n",
    "    \n",
    "    # Condition features\n",
    "    cond_features = get_condition_features(condition_str)\n",
    "    structured_features.update(cond_features)\n",
    "    \n",
    "    tokenized = {\n",
    "        \"title\": tf.convert_to_tensor(title_tokenizer(tf.constant(title))),\n",
    "        \"description\": tf.convert_to_tensor(desc_tokenizer(tf.constant(description)))\n",
    "    }\n",
    "    \n",
    "    img_tensor = process_image(tf.constant(image_url))\n",
    "\n",
    "    structured_tensors = {\n",
    "        key: tf.expand_dims(tf.convert_to_tensor(value, dtype=tf.float32), axis=0)\n",
    "        for key, value in structured_features.items()\n",
    "    }\n",
    "    \n",
    "    model_inputs = {\n",
    "        \"image_url\": tf.expand_dims(img_tensor, axis=0),\n",
    "        \"title\": tf.expand_dims(tokenized[\"title\"], axis=0),\n",
    "        \"description\": tf.expand_dims(tokenized[\"description\"], axis=0),\n",
    "        **structured_tensors\n",
    "    }\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     13\u001b[39m sample_date_sold = \u001b[33m\"\u001b[39m\u001b[33m2025-07-01\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 200 dollar real price, predicted is 204.48\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Prepare inputs\u001b[39;00m\n\u001b[32m     17\u001b[39m inference_inputs = prepare_inference_inputs(\n\u001b[32m     18\u001b[39m     title=sample_title,\n\u001b[32m     19\u001b[39m     description=sample_desc,\n\u001b[32m     20\u001b[39m     image_url=sample_image_url,\n\u001b[32m     21\u001b[39m     date_sold_str=sample_date_sold,\n\u001b[32m     22\u001b[39m     condition_str=sample_condition,\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     title_tokenizer=\u001b[43mtitle_tokenizer\u001b[49m,\n\u001b[32m     24\u001b[39m     desc_tokenizer=desc_tokenizer\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Get prediction\u001b[39;00m\n\u001b[32m     28\u001b[39m prediction_log_price = model.predict(inference_inputs)\n",
      "\u001b[31mNameError\u001b[39m: name 'title_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "# sample_title = \"Xbox Series X\"\n",
    "# sample_desc = \"Used for 1 day only then decided to sell because pc has more games\"\n",
    "# sample_condition = \"BRAND NEW\"\n",
    "# sample_image_url = \"https://media.karousell.com/media/photos/products/2025/5/13/xbox_series_x_1747134184_1cd82594_thumbnail.jpg\"\n",
    "# sample_date_sold = \"2025-05-14\"\n",
    "# 385 dollar real price, predicted is 372.19\n",
    "\n",
    "sample_title = \"Nintendo New 3DS\"\n",
    "sample_desc = \"Selling off this New 3DS (Non-XL). Comes with:- Console- Stylus- 32GB MicroSD- Charger Please Note: Device is modded, so can download any and all games from online. It also has a non-functioning volume slider, so the volume slider can't control the volume. Tried replacing the speakers and the cables but no luck in fixing this. However, because the console is modded the volume can still be adjusted anytime from the mod menu. Please purchase after careful consideration. Any questions feel free to ask, happy to answer any questions.\"\n",
    "sample_condition =\"HEAVILY USED\"\n",
    "sample_image_url = \"https://media.karousell.com/media/photos/products/2025/6/4/nintendo_new_3ds_1749051938_84b4b4d4_thumbnail\"\n",
    "sample_date_sold = \"2025-07-01\"\n",
    "# 200 dollar real price, predicted is 204.48\n",
    "\n",
    "# Prepare inputs\n",
    "inference_inputs = prepare_inference_inputs(\n",
    "    title=sample_title,\n",
    "    description=sample_desc,\n",
    "    image_url=sample_image_url,\n",
    "    date_sold_str=sample_date_sold,\n",
    "    condition_str=sample_condition,\n",
    "    title_tokenizer=title_tokenizer,\n",
    "    desc_tokenizer=desc_tokenizer\n",
    ")\n",
    "\n",
    "# Get prediction\n",
    "prediction_log_price = model.predict(inference_inputs)\n",
    "predicted_price = np.expm1(prediction_log_price[0][0])\n",
    "\n",
    "print(f\"Predicted price for the Xbox Series X: ${predicted_price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'sequential_6' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'sequential_7' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "inf_model = load_model(\n",
    "    \"price_predictor_v3.keras\",\n",
    "    custom_objects={\n",
    "        \"PricePredictor\": PricePredictor,\n",
    "        \"CNNImageEncoder\": CNNImageEncoder,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "Predicted price for the Xbox Series X: $372.19\n"
     ]
    }
   ],
   "source": [
    "check_prediction_log_price = inf_model.predict(inference_inputs)\n",
    "check_predicted_price = np.expm1(prediction_log_price[0][0])\n",
    "\n",
    "print(f\"Predicted price for the Xbox Series X: ${check_predicted_price:.2f}\")\n",
    "assert check_predicted_price == predicted_price, \"Predictions between save and mmodel in memory do not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output swings, depending on condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "Predicted price for Nintendo New 3DS, with condition WELL USED: $244.18\n"
     ]
    }
   ],
   "source": [
    "sample_title = \"Nintendo New 3DS\"\n",
    "sample_desc = \"Selling off this New 3DS (Non-XL). Comes with:- Console- Stylus- 32GB MicroSD- Charger Please Note: Device is modded, so can download any and all games from online. It also has a non-functioning volume slider, so the volume slider can't control the volume. Tried replacing the speakers and the cables but no luck in fixing this. However, because the console is modded the volume can still be adjusted anytime from the mod menu. Please purchase after careful consideration. Any questions feel free to ask, happy to answer any questions.\"\n",
    "sample_condition =\"WELL USED\"\n",
    "sample_image_url = \"https://media.karousell.com/media/photos/products/2025/6/4/nintendo_new_3ds_1749051938_84b4b4d4_thumbnail\"\n",
    "sample_date_sold = \"2025-07-01\"\n",
    "\n",
    "'''\n",
    "BRAND NEW 426.60\n",
    "LIKE NEW 384.40\n",
    "LIGHTLY USED 303.40\n",
    "WELL USED 244.18\n",
    "HEAVILY USED 204.48\n",
    "'''\n",
    "\n",
    "inference_inputs = prepare_inference_inputs(\n",
    "    title=sample_title,description=sample_desc,image_url=sample_image_url,date_sold_str=sample_date_sold,condition_str=sample_condition,title_tokenizer=title_tokenizer,desc_tokenizer=desc_tokenizer\n",
    ")\n",
    "\n",
    "prediction_log_price = inf_model.predict(inference_inputs)\n",
    "predicted_price = np.expm1(prediction_log_price[0][0])\n",
    "\n",
    "print(f\"Predicted price for {sample_title}, with condition {sample_condition}: ${predicted_price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 677ms/step - loss: 0.7100 - mean_absolute_error: 0.6843 - mean_squared_error: 1.1262 - root_mean_squared_error: 1.0571\n",
      "Test Loss: 0.713158130645752\n",
      "Test Mean Absolute Error: 0.6875342726707458\n",
      "Test RMSE: 1.0649105310440063\n",
      "Test MSE: 1.1340343952178955\n",
      "[0.713158130645752, 0.6875342726707458, 1.0649105310440063, 1.1340343952178955]\n"
     ]
    }
   ],
   "source": [
    "inf_model.compile(\n",
    "    loss='mean_absolute_error',\n",
    "    metrics=[\n",
    "        'mean_absolute_error',\n",
    "        tf.keras.metrics.RootMeanSquaredError(),\n",
    "        tf.keras.metrics.MeanSquaredError(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_ds_results = inf_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Test Loss:\", test_ds_results[0])\n",
    "print(\"Test Mean Absolute Error:\", test_ds_results[1])\n",
    "print(\"Test RMSE:\", test_ds_results[2])\n",
    "print(\"Test MSE:\", test_ds_results[3])\n",
    "print(test_ds_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is test loss different than MAE\n",
    "loss is computed per batch, then averaged over all batches\n",
    "\n",
    "mae and other metrics are calculated globally, so no averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prices_from_csv(file_path, model, title_tokenizer, desc_tokenizer):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.fillna(\"\")\n",
    "    for col in ['title', 'description', 'condition', 'image_url', 'date_sold']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).fillna('')\n",
    "\n",
    "    predicted_prices = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sample_title = row['title']\n",
    "        sample_desc = row['description']\n",
    "        sample_condition = row['condition']\n",
    "        sample_image_url = row['image_url']\n",
    "        sample_date_sold = row['date_sold']\n",
    "\n",
    "        inference_inputs = prepare_inference_inputs(\n",
    "            title=sample_title,\n",
    "            description=sample_desc,\n",
    "            image_url=sample_image_url,\n",
    "            date_sold_str=sample_date_sold,\n",
    "            condition_str=sample_condition,\n",
    "            title_tokenizer=title_tokenizer,\n",
    "            desc_tokenizer=desc_tokenizer\n",
    "        )\n",
    "\n",
    "        prediction_log_price = model.predict(inference_inputs)\n",
    "        predicted_price = np.expm1(prediction_log_price[0][0])\n",
    "        predicted_prices.append(predicted_price)\n",
    "\n",
    "    df['price'] = predicted_prices\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'sequential_6' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/aryan/Downloads/ds_assignment_2025/pricing-venv/lib/python3.11/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'sequential_7' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "334\n"
     ]
    }
   ],
   "source": [
    "result_df = predict_prices_from_csv('datasets/test.csv', inf_model, title_tokenizer, desc_tokenizer)\n",
    "\n",
    "result_df = result_df[['product_id', 'price']]\n",
    "print(len(result_df))\n",
    "result_df.to_csv('predicted_prices_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pricing-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
